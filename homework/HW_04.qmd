---
title: "Homework 4: ACS Income Prediction"
format:
  html:
    embed-resources: true
execute:
  python: env/bin/python3.11
number-sections: false
---

This homework assignment guides you through building and evaluating machine learning models to predict individual income using the American Community Survey (ACS) microdata. You will implement both an XGBoost model and a fully connected neural network using Keras, following the workflows demonstrated in the provided tutorials. The focus will be on log-transforming the income target, feature engineering, model training, and evaluation.

## Setup

```{r}
rm(list=ls())
```

The below section was generated by the GitHub chat agent after several hours of trying to get Tensorflow working on my RAND laptop.
```{r}
Sys.setenv(RETICULATE_PYTHON = "C:/r-tf/Scripts/python.exe")
#logfile <- "submit.log"
#sink(logfile, append = FALSE, split = TRUE)
rm(list=ls())

if(Sys.getenv("RETICULATE_PYTHON") == "") {
  Sys.setenv(RETICULATE_PYTHON = "C:/Users/jgluckman/AppData/Local/r-reticulate/r-reticulate/pyenv/pyenv-win/versions/3.11.9/python.exe")
} else {
   #cat("Using RETICULATE_PYTHON=", Sys.getenv("RETICULATE_PYTHON"), "\n")
  }

```

```{r}
pacman::p_load(xgboost, neuralnet, keras3, reticulate, tensorflow, tidyverse, rio)
```

More Chat input
```{r}
py_path <- Sys.getenv("RETICULATE_PYTHON")
if(py_path != "") {
  use_python(py_path, required = TRUE)
 } else {
   use_python("C:\\Users\\jgluckman\\AppData\\Local\\r-reticulate\\r-reticulate\\pyenv\\pyenv-win\\versions\\3.11.9\\python.exe", required=TRUE)  
}
```


```{r}
setwd("C:\\Users\\jgluckman\\OneDrive - RAND Corporation\\Desktop\\Machine Learning\\PRGS-Intro-to-ML-2025\\")
data <- import("data\\raw\\usa_00005.dta")
data.back <- data
```


Additional Chat fix for Python package errors
```{r}
if(!"keras" %in% loadedNamespaces()) {
   tryCatch(library(keras), error = function(e) print(paste("Warning: could not load keras: ", conditionMessage(e), "\n")))
 }
if(!"tensorflow" %in% loadedNamespaces()) {
   tryCatch(library(tensorflow), error = function(e) print(paste("Warning: could not load tensorflow: ", conditionMessage(e), "\n")))
 }
 if(!exists("keras_model_sequential")) {
   if(exists("keras_model_sequential", where = asNamespace("keras"), inherits = FALSE)) {
     assign("keras_model_sequential", get("keras_model_sequential", envir = asNamespace("keras")), envir = .GlobalEnv)
   }
 }
```


### Data
Use the ACS microdata file `usa_00005.dta` that I shared on Populi. This dataset contains individual-level records with demographic and socioeconomic features, including total personal income (`inctot`).


## Task 1: Load and Log-transform ACS Data

Following the final attempt in the XGBoost tutorial, sample 10% of the microdata, drop any rows where `inctot == 9999999`.

```{r}
sel <- sample(1:10, nrow(data), replace = TRUE)
data <- data[sel == 1,]
data <- data[!(data$inctot == 9999999),]
```

__Quesion 1:__ Why is it not necessary to impute or filter our missing values for other features?

WE ARE ONLY WORKING WITH THIS COLUMN AS THE DV

### Task 1.1: Log-transform Income

Instead of predicting income directly, we will predict log-income.
Create a new target column `log_inctot` by applying the natural log transformation to `inctot` after adding 1 (i.e., `np.log1p`).
Filter out any rows where `inctot` is less than or equal to zero before applying the log transformation.
Make a histogram of both the original income and log-income distributions to visualize the effect of the transformation.

```{r}
data <- data[data$inctot > 0,]
data$log_inctot <- log1p(data$inctot + 1)
ggplot(data) +
  geom_histogram(aes(x = inctot)) +
  labs(title = "Total Income Histogram")
ggplot(data) +
  geom_histogram(aes(x = log_inctot)) +
  labs(title = "Total Log Income Histogram")
write.csv(data, "cleaned.csv")
```

__Question 2:__ What are the advantages of predicting log-income rather than income directly?
Log_income better centralizes results into a more even range, instead of containing a long tail for the extreme end of the data.

## Task 2: Feature Engineering and Encoding
Split the data into a feature matrix `X` and target vector `y` (the log-income column created above). Perform one-hot encoding on all categorical features, dropping sparse columns whose proportion of non-zero entries falls below 1%.
The feature matrix `X` should include all columns except `inctot` and `log_inctot`.
The target vector `y` should be the `log_inctot` column.
Mimic the one-hot encoding workflow from the tutorial: build dummies for every categorical column in `X`, then drop sparse columns whose proportion of non-zero entries falls below 1%.

NOTE: CAN I DO THIS IN R?
```{r}
data <- read.csv("cleaned.csv")
X <- data[,-c(data$inctot, data$log_inctot)]
X <- X[colMeans(X) >= 0.01,]
y <- data$log_inctot
```

### Task 2.1: Train/Test Split
Split the encoded data into training and testing sets (80/20).

```{r}
train.perc <- 0.8
train.size <- round(train.perc*nrow(data))
test.size <- nrow(data) - train.size
assigns <- sample(0:1, prob = c(train.perc, (1-train.perc)))
train.y <- y[assigns == 0]
test.y <- as.numeric(y[assigns == 1])
train.x <- as.matrix(X[assigns == 0,])
test.x <- as.matrix(X[assigns == 1,])
```

## Task 3: XGBoost Modeling (Log Target)
Reproduce the final modeling attempt from `examples/XGBoost.ipynb`, but treat the log-income target as the label. Work through the following subtasks and insert your own code in each block.

### Task 3.1: Build DMatrix Objects
Create `xgb.DMatrix` objects for both the training and test splits using the log-income labels.

```{r}
xgb.train <- xgb.DMatrix(data = train.x, label = train.y)
xgb.test <- xgb.DMatrix(data = test.x, label = test.y)
```

### Task 3.2: Hyperparameter Dictionary
Start from the tutorialâ€™s baseline settings (e.g., `eta = 1`, `max_depth = 10`, `min_child_weight = 100`, RMSE metric, early stopping after 50 rounds) and justify any deviations you make.
Use the `reg:squarederror` objective and `rmse` evaluation metric.

R runs the training first and then adds parameters
```{r}
xgb.train <- xgboost(xgb.train, params = list(eta = 1, max_depth = 10), nrounds = 10000, verbose = FALSE, metrics = "rmse", objective = "reg:squarederror")
```

### Task 3.3: Hyperparameter Tuning

Perform a grid search over the `eta = [1, 0.1, 0.01]` and `max_depth = [3, 6, 10]` parameters to identify the best combination based on validation RMSE (i.e., for each combination of `eta` and `max_depth`, train a model and record the validation RMSE).
Use early stopping with a patience of 50 rounds and a maximum of 10,000 training rounds.

```{r}
etas <- c(1, 0.1, 0.1)
depths <- c(3, 6, 10)
finals <- cbind.data.frame(c(0, 0, 0), c(0, 0, 0), c(0, 0, 0))
names(finals) <- etas
rownames(finals) <- depths
for(i in 1:length(etas)){
  for(j in 1:length(depths)){
    xgb.tmp <- xgboost(train.x, label = train.y, params = list(eta = etas[i], max_depth = depths[j]), nrounds = 10000, verbose = FALSE, metrics = "rmse", objective = "reg:squarederror")
    finals[j, i] <- xgb.tmp$evaluation_log[nrow(xgb.tmp$evaluation_log)][[2]]
  }
}
print(finals)
```

__Question 3:__ Which hyperparameter combination yielded the lowest validation RMSE, and what was that RMSE value?
Based on the results, do you think you need to expand the grid search other values or parameters?
(Note: You do not need to actually search for more parameters, just discuss.)

Higher eta and higher depth are the most hyperspecific outputs, which will overfit results but remove rmse

### Task 3.4: Final Model Training
Using the best hyperparameters from Task 3.3, train a final XGBoost model on the full dataset.

```{r}
datamatrix <- xgb.DMatrix(data = as.matrix(X), label = data$log_inctot)
xgb.fin <- xgboost(data = datamatrix, label = train.y, params = list(eta = 1, max_depth = 10), nrounds = 10000, verbose = FALSE, metrics = "rmse", objective = "reg:squarederror")

```


### Task 3.5: Evaluate Predictions
Plot predicted vs. actual log-income values on the test set using a scatter plot.

```{r}
preds.xgb <- predict(xgb.fin, newdata = test.x)
mse <- sum((preds.xgb-test.y)^2)/length(preds.xgb)
slope <- mean(test.y/preds.xgb)
ggplot() +
geom_point(aes(x = preds.xgb, y = test.y)) +
geom_abline(slope = slope, color = "blue3") +
labs(title = "Predicted vs. Actual Values", subtitle = "XGBoost", x = "Predicted", y = "Actual")
```

__Question 4:__ Qualitatively, how well do the predictions align with the actual values?
The predictions align strongly, and according to the math, perfectly.

## Task 4: Neural Network Modeling (Keras)
Use the structure from `examples/neural_nets_simple.ipynb` to fit a fully connected neural network on the same encoded features and log-income target.
Use the same train/test split as in Task 2.1.

### Task 4.1: Prepare Data for Keras
If necessary, convert the training and test feature matrices and target vectors into NumPy arrays suitable for Keras.
__Note:__ You may have already done this in Task 2.1.

```{r}
head(train.x)
head(test.x)
head(train.y)
head(test.y)
```


### Task 4.2: Define the Architecture
Implement a dense network with several ReLU-activated hidden layers (e.g., 256-128-64) and compile it with the Adam optimizer and MSE loss.

__Question 5:__ What should the input dimension be for the first layer of the network be?

```{r}
mean.list.train <- lapply(data.frame(train.x), mean)
mean.list.test <- lapply(data.frame(test.x), mean)
sds.train <- sapply(data.frame(train.x), sd)
sds.test <- sapply(data.frame(test.x), sd)
train.xs <- scale(train.x, center = mean.list.train, scale = sds.train)
test.xs <- scale(test.x, center = mean.list.test, scale = sds.test)
model.a <- keras_model_sequential()
model.a %>% 
layer_dense(units = 256, activation = "relu") %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model.a %>% compile(optimizer = "adam", loss = "mse")
```

### Task 4.3: Initial Training
Fit the model on the training data while validating on the test set.
Run the training for a maximum of 1,000 epochs.
```{r}
model.a %>% fit(x = train.xs, y = train.y, epochs = 1000, batch_size = 1024, validation_split = 0.2, verbose = 0)
```
I have some example code below that includes early stopping to prevent overfitting.
You may adjust as needed.
The `patience` parameter in the `EarlyStopping` callback controls how many epochs with no improvement on the validation loss to wait before stopping training.
The `batch_size` is a parameter that we haven't discussed in class, but it limits the amount of data the model is exposed to at once during training.
You can leave it at 1024 for this assignment.

#### Task 4.3a
Plot the training and validation loss curves over epochs.

```{r}
preds.nn <- model.a$predict(test.xs)
mse <- as.numeric(mean(loss_mean_squared_error(preds.nn, test.y)))
slope <- mean(test.y/preds.nn)
ggplot() +
geom_point(aes(x = preds.nn, y = test.y)) +
geom_abline(slope = slope, color = "blue3") +
labs(title = "Predicted vs. Actual Values", subtitle = "Neural Network", x = "Predicted", y = "Actual")
```

__Question 6:__ Based on the loss curves, does the model appear to be underfitting, overfitting, or well-fitted? Justify your answer.
The model seems to fit pretty well, with the data clustered along the intended line.

### Task 4.4: Model Tuning
The neural network architecture and training parameters can significantly impact performance.
The architecture consists of the number of layers, layer dimensions, and activation functions.
Create three different architectures: the one from Task 4.2, a deeper/wider one, and a shallower/narrower one.
Additionally, consider three different learning rates: `1e-3`, `1e-4`, and `1e-5`.
Perform a grid search over these architectures and learning rates to identify the best combination based on validation loss.
__Hint:__ Save the models and their histories in a dictionary for easy comparison later.

```{r}
architectures <- c("classic", "deep", "shallow")
rates <- c(.001, .0001, .00001)
table <- cbind.data.frame(c(0, 0, 0), c(0, 0, 0), c(0, 0, 0))
names(table) <- architectures
rownames(table) <- rates
models <- list()
for(i in 1:nrow(table)){
  model.loc <- keras_model_sequential()
  model.loc %>% 
    layer_dense(units = 256, activation = "relu") %>%
    layer_dense(units = 128, activation = "relu") %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 1)
  model.loc %>% compile(optimizer = optimizer_adam(learning_rate = rates[i]), loss = "mse")
  preds.tmp <- model.loc$predict(test.xs)
  mse.tmp <- as.numeric(mean(loss_mean_squared_error(preds.tmp, test.y)))
  models[[i]] <- model.loc
  table[i, 1] <- mse.tmp
}

for(i in 1:nrow(table)){
  model.loc <- keras_model_sequential()
  model.loc %>% 
    layer_dense(units = 256, activation = "relu") %>%
    layer_dense(units = 128, activation = "relu") %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 16, activation = "relu") %>%
    layer_dense(units = 1)
  model.a %>% compile(optimizer = optimizer_adam(learning_rate = rates[i]), loss = "mse")
  preds.tmp <- model.loc$predict(test.xs)
  mse.tmp <- as.numeric(mean(loss_mean_squared_error(preds.tmp, test.y)))
  models[[i + 3]] <- model.loc
  table[i, 2] <- mse.tmp
}

for(i in 1:nrow(table)){
  model.loc <- keras_model_sequential()
  model.loc %>% 
    layer_dense(units = 64, activation = "relu") %>%
    layer_dense(units = 32, activation = "relu") %>%
    layer_dense(units = 1)
  model.a %>% compile(optimizer = optimizer_adam(learning_rate = rates[i]), loss = "mse")
  preds.tmp <- model.loc$predict(test.xs)
  mse.tmp <- as.numeric(mean(loss_mean_squared_error(preds.tmp, test.y)))
  models[[i + 6]] <- model.loc
  table[i, 3] <- mse.tmp
}
print(table)
```


__Question 7:__ Which architecture and learning rate combination yielded the lowest validation loss, and what was that loss value?
The lowest validation loss came from the classic model with the highest learning rate (0.001), for a loss of 95.7881


### Task 4.5: Evaluate Predictions
Generate predictions on the test set using the best model from Task 4.
Create a scatter plot of predicted vs. actual log-income values.

```{r}
model.b <- models[[1]]
preds.b <- model.b$predict(test.xs)
mse.b <- table[1, 1]
slope <- mean(test.y/preds.b)
ggplot() +
geom_point(aes(x = preds.b, y = test.y)) +
geom_abline(slope = slope, color = "blue3") +
labs(title = "Predicted vs. Actual Values for Classic Model", subtitle = "Neural Network, Learning Rate 1e-3", x = "Predicted", y = "Actual")
```


## Task 5: Model Comparison and Reflection
Which model would you choose for predicting log-income on new ACS data: the XGBoost model from Task 3 or the neural network from Task 4? Why?

I prefer using XGBoost, largely because the results come out more linear on a prediction. I suspect that the problem with the Neural Net model is the result of overfitting and bias, which is probably mitigatable with a different learning rate, but not in the scope of this problem set.