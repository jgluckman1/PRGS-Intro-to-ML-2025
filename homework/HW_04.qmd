---
title: "Homework 4"
format: 
    html:
        embed-resources: true
---


__Due Date:__ 2024-11-13 at 8:30 AM PT
---


__Name:__ JJ Gluckman


## Preparation

1. Download the [data file](https://github.com/gabehassler/PRGS-Intro-to-ML-2024/blob/main/data/processed/svi_covid.csv) from GitHub and place it in the _data/processed_ folder.



## Homework - Regression Trees

The goal of this analysis is to use a regression tree to predict the number of per-capita COVID-19 deaths in each county in the US using the SVI variables.

1. Load the data file.
```{r}
rm(list=ls())
pacman::p_load(rpart, caret)
data <- read.csv("C:\\Users\\jgluckman\\OneDrive - RAND Corporation\\Desktop\\Machine Learning\\PRGS-Intro-To-ML-2025\\data\\processed\\svi_covid.csv")
View(data)
```

2. Write the following functions:
    - A function that fits a regression tree to data. The function should take as input the data, the outcome variable, the predictor variables, and the maximum depth of the tree. The function should return the fitted tree.
    _Note: Many packages have functions that penalize the complexity of the tree to avoide overfitting. You should make sure that the function you write does not use any penalization for the complexity of the tree._
    - A function that predicts the outcome variable using a fitted tree. The function should take as input the fitted tree and the data for which to make predictions. The function should return the predicted values.
    - A function that calculates the mean squared error of the predictions. The function should take as input the predicted values and the true values. The function should return the mean squared error.

```{r}
fips <- data$fips_code
trim <- data[3:ncol(data)]
tree <- rpart(trim)
tree <- rpart(total_deaths_per_100k ~ EP_POV150 + EP_UNEMP + EP_HBURD + EP_NOHSDP + EP_UNINSUR + EP_AGE65 + EP_AGE17 + EP_DISABL + EP_SNGPNT + EP_LIMENG + EP_MINRTY + EP_MUNIT + EP_MOBILE + EP_CROWD + EP_NOVEH + EP_GROUPQ + EP_NOINT, data = trim)
plot(tree)
text(tree)
```

```{r}
outcomes <- predict(tree)
```


```{r}
preds <- c("EP_POV150", "EP_UNEMP", "EP_HBURD", "EP_NOHSDP", "EP_UNINSUR", "EP_AGE65", "EP_AGE17", "EP_DISABL", "EP_SNGPNT", "EP_LIMENG", "EP_MINRTY", "EP_MUNIT", "EP_MOBILE", "EP_CROWD", "EP_NOVEH", "EP_GROUPQ", "EP_NOINT")
rms.create <- function(x, t = preds, s = trim){
    val <- rep(NA, length(t))
    for(i in 1:length(t)){
        pred.mean <- cbind.data.frame(x$centers[,i])
        pred.mean$rel <- seq(1,5)
        names(pred.mean) <- c("mean", "ids")
        split <- s[,i + 1]
        ids <- x$cluster
        split <- cbind.data.frame(split, ids)
        split <- merge(split, pred.mean, by = "ids")
        diff2 <- (split$split - split$mean)^2
        val[i] <- sqrt(sum(diff2))/nrow(split)
    }
    gmean <- mean(val)
    rmse <- sum(sqrt((val-gmean)^2/length(preds)))
    return(rmse)
}
```


3. Use 5-fold cross-validation to calculate the mean squared error of the regression tree for maximum tree depths 1, ..., 10.
The outcome variable is `total_deaths_per_100k` and the predictor variables are `EP_POV150, EP_UNEMP, EP_HBURD, EP_NOHSDP, EP_UNINSUR, EP_AGE65, EP_AGE17, EP_DISABL, EP_SNGPNT, EP_LIMENG, EP_MINRTY, EP_MUNIT, EP_MOBILE, EP_CROWD, EP_NOVEH, EP_GROUPQ, EP_NOINT`.
```{r}
k5 <- kmeans(trim[,preds], centers = 5, iter.max = 10)
rmse <- rms.create(k5)
rmse
```

4. Plot the mean squared error as a function of the maximum tree depth.
```{r}
rmse.fun <- cbind.data.frame(rep(NA, length(preds)))
for(i in 1:length(preds)){
    k.loc <- kmeans(trim[,preds], centers = 5, iter.max = 100)
    rmse.fun[i,] <- rms.create(k.loc)
}
rmse.fun$xl <- seq(1, length(preds))
names(rmse.fun) <- c("val", "row")

ggplot(data = rmse.fun) +
    geom_line(aes(x = row, y = val)) +
    labs(title = "RMSE with Depth Level", x = "Depth", y = "Value")
```
5. Which maximum tree depth would you choose based on the cross-validation results? Why?
Reliability seems to drop wildly after a depth of 3, so that seems to be a reasonable cap.

6. Fit a regression tree to the full data using the maximum tree depth you chose in the previous question.
```{r}
tree2 <- rpart(total_deaths_per_100k ~ EP_POV150 + EP_UNEMP + EP_HBURD + EP_NOHSDP + EP_UNINSUR + EP_AGE65 + EP_AGE17 + EP_DISABL + EP_SNGPNT + EP_LIMENG + EP_MINRTY + EP_MUNIT + EP_MOBILE + EP_CROWD + EP_NOVEH + EP_GROUPQ + EP_NOINT, data = trim, maxdepth = 3)
```

7. Plot the fitted tree. Summarize the tree in words. What variables seem to be the most important predictors of the number of per-capita COVID-19 deaths?
```{r}
plot(tree2)
text(tree2)
```

NOINT, POV150, DISABL, and HSDP are the largest predictors.

8. Plot the predicted values against the true values. How much would you trust the predictions of the regression tree? Why?
```{r}
predictor <- predict(tree2)
ggplot() +
    geom_point(aes(x = trim$total_deaths_per_100k, y = predictor)) +
    geom_smooth(aes(x = trim$total_deaths_per_100k, y = predictor)) +
    labs(title = "Prediction vs. Actual", x = "Actual Deaths per 100k", y = "Predicted Deaths per 100k")

```

The regression pattern seems to show a range where the prediction works, but with very long tails on both ends, which made me run the following distribution check on my predicted values

```{r}
ggplot(data = data.frame(predictor)) +
    geom_histogram(aes(x = predictor), stat = "count")
```

This spread clarified that my existing breakdown is overzealous with properly defining subclasses, which renders the predictions wildly unhelpful. It's probably more useful to use more splits, but that would require compromising on RMSE, unless I messed up my RMSE math.